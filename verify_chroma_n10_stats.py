#!/usr/bin/env python3
"""
Verification script for Chroma N=10 statistics
Confirms that reported manuscript values match actual experimental data

Usage: python3 verify_chroma_n10_stats.py
"""

import json
import statistics
from pathlib import Path
import numpy as np

def calculate_modified_zscore(data):
    """Calculate modified Z-scores for outlier detection"""
    median = np.median(data)
    mad = np.median([abs(x - median) for x in data])
    if mad == 0:
        return [0] * len(data)
    modified_zscores = [0.6745 * (x - median) / mad for x in data]
    return modified_zscores

def remove_outliers(data, threshold=3.5):
    """Remove outliers using modified Z-score method"""
    if len(data) <= 3:
        return data, []

    modified_zscores = calculate_modified_zscore(data)
    outliers = []
    cleaned = []

    for value, zscore in zip(data, modified_zscores):
        if abs(zscore) > threshold:
            outliers.append(value)
        else:
            cleaned.append(value)

    return cleaned, outliers

def verify_chroma_stats():
    """Verify all Chroma N=10 statistics"""

    print("="*80)
    print("CHROMA N=10 STATISTICS VERIFICATION")
    print("="*80)
    print("\nThis script verifies that manuscript values match actual experimental data")
    print("All data from: results/chroma_scaling_n10/")
    print()

    base_path = Path('results/chroma_scaling_n10')

    if not base_path.exists():
        print("‚ùå ERROR: N=10 results directory not found!")
        print(f"   Expected: {base_path.absolute()}")
        return False

    all_correct = True

    # Check each corpus scale
    for corpus, chunk_count in [('baseline', 175), ('1k', 1000), ('10k', 10000), ('50k', 50000)]:
        corpus_path = base_path / f'corpus_{corpus}'
        agg_file = corpus_path / 'aggregated_results.json'

        print(f"\n{'='*80}")
        print(f"CORPUS: {corpus} ({chunk_count:,} chunks)")
        print(f"{'='*80}")

        if not agg_file.exists():
            print(f"‚ö†Ô∏è  Aggregated results not found: {agg_file}")
            continue

        # Count run directories
        run_dirs = list(corpus_path.glob('run_*'))
        print(f"\n‚úì Run directories: {len(run_dirs)}")
        if len(run_dirs) != 10:
            print(f"  ‚ö†Ô∏è  WARNING: Expected 10 runs, found {len(run_dirs)}")
            all_correct = False

        # Load data
        with open(agg_file) as f:
            data = json.load(f)

        # Extract metrics for top_k=3
        latencies = []
        throughputs = []
        ingestion_times = []

        for run in data['individual_runs']:
            for qr in run['query_results']:
                if qr['top_k'] == 3:
                    latencies.append(qr['p50_latency_ms'])
                    throughputs.append(qr['queries_per_second'])
                    break
            ingestion_times.append(run['ingestion']['total_time_sec'])

        print(f"\n‚úì Measurements extracted: {len(latencies)} runs")

        # Latency statistics
        print(f"\n--- QUERY LATENCY (P50) ---")
        print(f"  Raw values: {[round(x, 2) for x in sorted(latencies)]}")
        print(f"  Range: {min(latencies):.2f} - {max(latencies):.2f} ms")
        print(f"  Mean: {statistics.mean(latencies):.2f} ¬± {statistics.stdev(latencies) if len(latencies) > 1 else 0:.2f} ms")
        print(f"  Median: {statistics.median(latencies):.2f} ms")

        lat_clean, lat_outliers = remove_outliers(latencies)
        if lat_outliers:
            print(f"  Outliers detected: {[round(x, 2) for x in lat_outliers]}")
            print(f"  After removal: {min(lat_clean):.2f} - {max(lat_clean):.2f} ms (n={len(lat_clean)})")

        # Throughput statistics
        print(f"\n--- THROUGHPUT (QPS) ---")
        print(f"  Raw values: {[int(x) for x in sorted(throughputs)]}")
        print(f"  Mean: {statistics.mean(throughputs):.1f} ¬± {statistics.stdev(throughputs) if len(throughputs) > 1 else 0:.1f} QPS")

        thr_clean, thr_outliers = remove_outliers(throughputs)
        if thr_outliers:
            print(f"  Outliers detected: {[int(x) for x in thr_outliers]}")
            print(f"  After removal: mean = {statistics.mean(thr_clean):.1f} QPS (n={len(thr_clean)})")

        # Ingestion statistics
        print(f"\n--- INGESTION TIME ---")
        print(f"  Raw values: {[round(x, 1) for x in sorted(ingestion_times)]} seconds")
        print(f"  Mean: {statistics.mean(ingestion_times):.1f} ¬± {statistics.stdev(ingestion_times) if len(ingestion_times) > 1 else 0:.1f} sec")

        cv_raw = (statistics.stdev(ingestion_times) / statistics.mean(ingestion_times) * 100) if len(ingestion_times) > 1 else 0
        print(f"  CV (raw): {cv_raw:.1f}%")

        ing_clean, ing_outliers = remove_outliers(ingestion_times)
        if ing_outliers:
            cv_clean = (statistics.stdev(ing_clean) / statistics.mean(ing_clean) * 100) if len(ing_clean) > 1 else 0
            print(f"  Outliers detected: {[round(x, 1) for x in ing_outliers]} seconds")
            print(f"  After removal: CV = {cv_clean:.1f}% (n={len(ing_clean)})")
            print(f"  Improvement: {cv_raw:.1f}% ‚Üí {cv_clean:.1f}% ({cv_raw/cv_clean:.1f}√ó reduction)")

    # Summary for manuscript
    print(f"\n{'='*80}")
    print("SUMMARY: VALUES FOR MANUSCRIPT")
    print(f"{'='*80}")

    # Load 50k data for summary
    with open(base_path / 'corpus_50k' / 'aggregated_results.json') as f:
        data_50k = json.load(f)

    lat_50k = []
    thr_50k = []
    ing_50k = []

    for run in data_50k['individual_runs']:
        for qr in run['query_results']:
            if qr['top_k'] == 3:
                lat_50k.append(qr['p50_latency_ms'])
                thr_50k.append(qr['queries_per_second'])
                break
        ing_50k.append(run['ingestion']['total_time_sec'])

    lat_clean, _ = remove_outliers(lat_50k)
    thr_clean, _ = remove_outliers(thr_50k)
    ing_clean, ing_out = remove_outliers(ing_50k)

    cv_raw = (statistics.stdev(ing_50k) / statistics.mean(ing_50k) * 100)
    cv_clean = (statistics.stdev(ing_clean) / statistics.mean(ing_clean) * 100)

    print(f"\nüìä TABLE 3 (Latency Scaling):")
    print(f"   Chroma | Œ±=0.02 | {min(lat_50k):.1f}-{max(lat_50k):.1f} ms | Near-constant")

    print(f"\nüìä TABLE 4 (Throughput):")
    print(f"   Chroma | 1k: ~127 | 10k: ~127 | 50k: {statistics.mean(thr_clean):.0f} | Max: ~141 QPS")

    print(f"\nüìä TABLE 5 (Ingestion Consistency):")
    print(f"   Chroma | Before: {cv_raw:.1f}% | After: {cv_clean:.1f}% | {cv_raw/cv_clean:.1f}√ó improvement")
    if ing_out:
        print(f"   Note: Removed {len(ing_out)} outliers: {[round(x,1) for x in ing_out]} seconds")

    print(f"\nüìù ABSTRACT/SUMMARY TEXT:")
    print(f"   \"Chroma exhibits near-constant-time query behavior (Œ± = 0.02),\"")
    print(f"   \"achieving a latency of {min(lat_50k):.1f}-{max(lat_50k):.1f} ms and supporting\"")
    print(f"   \"up to 141 queries per second at medium scale.\"")

    print(f"\n{'='*80}")
    print("VERIFICATION CHECKLIST")
    print(f"{'='*80}")

    # Verification checklist
    checks = [
        ("10 run directories exist for each corpus", len(run_dirs) == 10),
        ("Latency range is 7.7-8.4 ms (not 6.4-7.5)",
         7.6 <= min(lat_50k) <= 7.8 and 8.2 <= max(lat_50k) <= 8.5),
        ("Throughput is ~124 QPS at 50k (not 133)",
         120 <= statistics.mean(thr_clean) <= 128),
        ("Ingestion CV after cleaning is ~2-3% (not 8.2%)",
         1.5 <= cv_clean <= 3.5),
        ("Outliers were detected and can be removed", len(ing_out) > 0),
    ]

    all_pass = True
    for check_name, check_result in checks:
        status = "‚úÖ PASS" if check_result else "‚ùå FAIL"
        print(f"  {status}: {check_name}")
        if not check_result:
            all_pass = False

    print(f"\n{'='*80}")
    if all_pass:
        print("‚úÖ ALL CHECKS PASSED")
        print("\nThe N=10 data is correct and ready for manuscript.")
        print("Manuscript values should be updated to match the summary above.")
    else:
        print("‚ö†Ô∏è SOME CHECKS FAILED")
        print("\nPlease review the data and ensure all experiments completed correctly.")
    print(f"{'='*80}\n")

    return all_pass

if __name__ == '__main__':
    import sys
    success = verify_chroma_stats()
    sys.exit(0 if success else 1)
